from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.vec_env import DummyVecEnv
import multiprocessing
import sys
import os
parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(parent_dir)
from gym_continuum_env import ContinuumRobotEnv
import torch
from torch import nn
torch.backends.cudnn.benchmark = True
policy_kwargs = dict(
    net_arch=[1024, 1024, 512],  # ğŸ§  è¶…å¤§ç½‘ç»œç»“æ„
)
if __name__ == "__main__":
    multiprocessing.set_start_method("spawn")  # å¯é€‰ï¼Œä½†å»ºè®®åŠ ä¸Š
# å¹¶è¡Œç¯å¢ƒï¼šæ ¹æ® CPU æ ¸å¿ƒæ¥é€‰ï¼ˆæ¨è 8ï½16ï¼‰
    env = make_vec_env(
        ContinuumRobotEnv,
        n_envs=30,  # å¹¶è¡Œ 8 ä¸ªç¯å¢ƒï¼ˆå¯è°ƒï¼‰
        vec_env_cls=SubprocVecEnv,
        env_kwargs={
        "control_mode": 0,      # âœ… ä½ è¦ä¼ çš„å˜é‡
        "render_mode": False
        }
    )

    # ä½¿ç”¨ GPU + å¤§ batch
    model = PPO(
        policy="MlpPolicy",
        env=env,
        device="cuda",
        policy_kwargs=policy_kwargs,
        learning_rate=1e-3,
        n_steps=512,         # æ¯ä¸ªç¯å¢ƒé‡‡æ · 4096 æ­¥ â†’ 8Ã—4096 = 32768 æ ·æœ¬
        batch_size=1024*3,     # æ¯æ¬¡è®­ç»ƒ 16384 æ ·æœ¬ï¼ˆåƒæ‰å¤§æ˜¾å­˜ï¼‰
        n_epochs=4,          # å¤šè½®è®­ç»ƒ        verbose=1,
                # âœ… é™åˆ¶ value å‡½æ•°æ›´æ–°
        tensorboard_log="./ppo_log/",
        verbose=0
    )
    #model = PPO.load("continuum_robot_ppo_2e7", env=env, device="cuda")
    model.learn(total_timesteps=1000000, progress_bar=True)
    model.save("continuum_robot_ppo_step")
    env.close()
