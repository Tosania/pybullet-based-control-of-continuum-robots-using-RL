from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
import multiprocessing
import sys
import os
parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(parent_dir)
from gym_continuum_env import ContinuumRobotEnv
import torch

policy_kwargs = dict(
    net_arch=[1024, 1024, 512],  # ğŸ§  è¶…å¤§ç½‘ç»œç»“æ„
)
if __name__ == "__main__":
    multiprocessing.set_start_method("spawn")  # å¯é€‰ï¼Œä½†å»ºè®®åŠ ä¸Š
# å¹¶è¡Œç¯å¢ƒï¼šæ ¹æ® CPU æ ¸å¿ƒæ¥é€‰ï¼ˆæ¨è 8ï½16ï¼‰
    env = make_vec_env(
        ContinuumRobotEnv,
        n_envs=24,  # å¹¶è¡Œ 8 ä¸ªç¯å¢ƒï¼ˆå¯è°ƒï¼‰
        vec_env_cls=SubprocVecEnv,
        env_kwargs={
        "control_mode": 1,      # âœ… ä½ è¦ä¼ çš„å˜é‡
        "render_mode": False
        }
    )

    # ä½¿ç”¨ GPU + å¤§ batch
    model = PPO(
        policy="MlpPolicy",
        env=env,
        device="cpu",
        policy_kwargs=policy_kwargs,
        n_steps=4096,         # æ¯ä¸ªç¯å¢ƒé‡‡æ · 4096 æ­¥ â†’ 8Ã—4096 = 32768 æ ·æœ¬
        batch_size=16384,     # æ¯æ¬¡è®­ç»ƒ 16384 æ ·æœ¬ï¼ˆåƒæ‰å¤§æ˜¾å­˜ï¼‰
        n_epochs=10,          # å¤šè½®è®­ç»ƒ
        verbose=0,
        tensorboard_log="./ppo_log/"
    )

    model.learn(total_timesteps=100000, progress_bar=True)
    model.save("continuum_robot_ppo")
    env.close()
