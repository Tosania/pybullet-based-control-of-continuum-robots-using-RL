from stable_baselines3.common.callbacks import BaseCallback
import os
import numpy as np
from gym_continuum_env import ContinuumRobotEnv
import threading
import queue

class SaveBestValueLossCallback(BaseCallback):
    def __init__(self, check_freq, save_path, target_path, device="cuda", control_mode=1, verbose=1):
        super(SaveBestValueLossCallback, self).__init__(verbose)
        self.check_freq = check_freq
        self.save_path = save_path
        self.best_error = float("inf")
        self.target_path = target_path
        self.device = device
        self.control_mode = control_mode
        self.best_reward = float("-inf")

    def _on_step(self):
        # æ¯ check_freq æ­¥è·å–ä¸€æ¬¡å¥–åŠ±
        if self.n_calls % self.check_freq == 0:
            
            # è·å–æœ€æ–°çš„å¹³å‡å¥–åŠ±
            if hasattr(self.model.env, 'get_attr'):
                mean_rewards = self.model.env.get_attr('mean_rewards')
                if len(mean_rewards) > 0:
                    # è®¡ç®—æ‰€æœ‰ç¯å¢ƒçš„å¹³å‡å¥–åŠ±
                    overall_mean_reward = np.mean(mean_rewards)
                    self.logger.record("train/mean_reward", overall_mean_reward)
                    
                    # æ›´æ–°æœ€ä½³å¥–åŠ±å¹¶ä¿å­˜æ¨¡å‹
                    if overall_mean_reward > self.best_reward:
                        self.best_reward = overall_mean_reward
                        if self.verbose > 0:
                            print(f"ğŸ“ˆ æ–°çš„æœ€é«˜å¥–åŠ±: {overall_mean_reward:.4f}ï¼Œæ­£åœ¨ä¿å­˜æ¨¡å‹...")
                        self.model.save(self.save_path)
        
        return True